This is federated training
[2024-05-08 14:34:22,582] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /home/nour.hesham/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m NVIDIA Inference is only supported on Ampere and newer architectures
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
experiment_name: first_trial_llavamed_from_without_tags
num_clients: 2
num_rounds: 1
data:
  name: Standard LLAVA Data Format
  full_path: /scratch/nshaheen/project/may_federated/LLaVA_flowertune/data_jsons/
  editable_path: train_tags_client_
  extension: .json
  image_folder: /scratch/nshaheen/project/may_federated/train_images/
  image_aspect_ratio: pad
  is_multimodal: true
  per_client_data_path: ''
  image_processor: ''
  mm_use_im_start_end: ''
model:
  mm_projector_lr: 2.0e-05
  name: /scratch/nshaheen/LLAMA/llava_med_in_text_60k_FINAL
  quantization: 4
  gradient_checkpointing: true
  freeze_backbone: false
  peft_lora_r: 128
  peft_lora_alpha: 256
  lora_enable: true
  lora_dropout: 0.05
  lora_bias: none
  model_max_length: 512
  vision_tower: openai/clip-vit-large-patch14-336
  tune_mm_mlp_adapter: false
  mm_vision_select_layer: -2
  mm_vision_select_feature: patch
  pretrain_mm_mlp_adapter: None
  mm_patch_merge_type: flat
  mm_use_im_patch_token: false
  mm_use_im_start_end: false
  mm_projector_type: linear
  freeze_mm_mlp_adapter: false
train:
  quantization: 4
  lazy_preprocess: true
  double_quant: true
  quant_type: nf4
  lora_r: 64
  lora_alpha: 16
  name: ''
  full_path: ''
  num_rounds: ${num_rounds}
  save_every_round: 5
  learning_rate_max: 5.0e-05
  learning_rate_min: 1.0e-06
  seq_length: 512
  group_by_modality_length: true
  mpt_attn_impl: triton
  training_arguments:
    output_dir: output/${experiment_name}
    report_to: tensorboard
    num_train_epochs: 1
    gradient_checkpointing: true
    fp16: true
    bf16: false
    per_device_train_batch_size: 4
    per_device_eval_batch_size: 4
    gradient_accumulation_steps: 1
    weight_decay: 0.0
    optim: adamw_torch
    remove_unused_columns: false
    evaluation_strategy: 'no'
    save_strategy: steps
    save_steps: 5000
    save_total_limit: 1
    learning_rate: 0.0002
    warmup_ratio: 0.03
    lr_scheduler_type: cosine
    logging_steps: 1
    dataloader_num_workers: 4
strategy:
  _target_: flwr.server.strategy.FedAvg
  fraction_fit: 0.1
  fraction_evaluate: 0.0
client_resources:
  num_cpus: 2
  num_gpus: 1.0

NUM CPUS: 2
[2024-05-08 14:34:28,658][flwr][INFO] - Starting Flower simulation, config: num_rounds=1, no round_timeout
[2024-05-08 14:34:56,917][flwr][INFO] - Flower VCE: Ray initialized with resources: {'object_store_memory': 55302650265.0, 'node:__internal_head__': 1.0, 'memory': 119039517287.0, 'CPU': 40.0, 'node:10.53.222.32': 1.0, 'accelerator_type:V100': 1.0, 'GPU': 1.0}
[2024-05-08 14:34:56,919][flwr][INFO] - Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html
[2024-05-08 14:34:56,921][flwr][INFO] - Flower VCE: Resources for each Virtual Client: {'num_cpus': 2, 'num_gpus': 1.0}
[2024-05-08 14:34:56,944][flwr][INFO] - Flower VCE: Creating VirtualClientEngineActorPool with 1 actors
[2024-05-08 14:34:56,946][flwr][INFO] - [INIT]
[2024-05-08 14:34:56,948][flwr][INFO] - Requesting initial parameters from one random client
[2m[36m(ClientAppActor pid=852642)[0m [2024-05-08 14:35:05,374] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2m[36m(ClientAppActor pid=852642)[0m Warning: The default cache directory for DeepSpeed Triton autotune, /home/nour.hesham/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2m[36m(ClientAppActor pid=852642)[0m [93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[2m[36m(ClientAppActor pid=852642)[0m [93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[2m[36m(ClientAppActor pid=852642)[0m [93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[2m[36m(ClientAppActor pid=852642)[0m [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[2m[36m(ClientAppActor pid=852642)[0m [93m [WARNING] [0m NVIDIA Inference is only supported on Ampere and newer architectures
[2m[36m(ClientAppActor pid=852642)[0m [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[2m[36m(ClientAppActor pid=852642)[0m [93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[2024-05-08 14:36:28,828][flwr][INFO] - Received initial parameters from one random client
[2024-05-08 14:36:28,830][flwr][INFO] - Evaluating initial global parameters
[2024-05-08 14:36:29,265][flwr][INFO] - initial parameters (loss, other metrics): 0.0, {}
[2024-05-08 14:36:29,267][flwr][INFO] - 
[2024-05-08 14:36:29,268][flwr][INFO] - [ROUND 1]
[2024-05-08 14:36:29,271][flwr][INFO] - configure_fit: strategy sampled 2 clients (out of 2)
[2024-05-08 14:37:22,390][flwr][ERROR] - Traceback (most recent call last):
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 73, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 399, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 280, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=852642, ip=10.53.222.32, actor_id=d658ca05287a7be6e99f355501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fa8b981cd90>)
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/flwr/client/client_app.py", line 98, in __call__
    return self._call(message, context)
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/flwr/client/client_app.py", line 81, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/flwr/client/message_handler/message_handler.py", line 130, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/scratch/nshaheen/project/may_federated/LLaVA_flowertune/client.py", line 76, in fit
    trainer = LLaVATrainer(model=self.model,
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/transformers/trainer.py", line 503, in __init__
    self.callback_handler = CallbackHandler(
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/transformers/trainer_callback.py", line 313, in __init__
    self.add_callback(cb)
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/transformers/trainer_callback.py", line 330, in add_callback
    cb = callback() if isinstance(callback, type) else callback
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/transformers/integrations/integration_utils.py", line 591, in __init__
    raise RuntimeError(
RuntimeError: TensorBoardCallback requires tensorboard to be installed. Either update your PyTorch version or install tensorboardX.

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=852642, ip=10.53.222.32, actor_id=d658ca05287a7be6e99f355501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fa8b981cd90>)
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: TensorBoardCallback requires tensorboard to be installed. Either update your PyTorch version or install tensorboardX.

[2024-05-08 14:37:22,392][flwr][ERROR] - [36mray::ClientAppActor.run()[39m (pid=852642, ip=10.53.222.32, actor_id=d658ca05287a7be6e99f355501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fa8b981cd90>)
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/flwr/client/client_app.py", line 98, in __call__
    return self._call(message, context)
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/flwr/client/client_app.py", line 81, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/flwr/client/message_handler/message_handler.py", line 130, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/scratch/nshaheen/project/may_federated/LLaVA_flowertune/client.py", line 76, in fit
    trainer = LLaVATrainer(model=self.model,
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/transformers/trainer.py", line 503, in __init__
    self.callback_handler = CallbackHandler(
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/transformers/trainer_callback.py", line 313, in __init__
    self.add_callback(cb)
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/transformers/trainer_callback.py", line 330, in add_callback
    cb = callback() if isinstance(callback, type) else callback
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/transformers/integrations/integration_utils.py", line 591, in __init__
    raise RuntimeError(
RuntimeError: TensorBoardCallback requires tensorboard to be installed. Either update your PyTorch version or install tensorboardX.

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=852642, ip=10.53.222.32, actor_id=d658ca05287a7be6e99f355501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fa8b981cd90>)
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: TensorBoardCallback requires tensorboard to be installed. Either update your PyTorch version or install tensorboardX.
[2024-05-08 14:38:07,314][flwr][ERROR] - Traceback (most recent call last):
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 73, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 399, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 280, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/ray/_private/worker.py", line 2524, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=852642, ip=10.53.222.32, actor_id=d658ca05287a7be6e99f355501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fa8b981cd90>)
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/flwr/client/client_app.py", line 98, in __call__
    return self._call(message, context)
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/flwr/client/client_app.py", line 81, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/flwr/client/message_handler/message_handler.py", line 130, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/scratch/nshaheen/project/may_federated/LLaVA_flowertune/client.py", line 84, in fit
    results = trainer.train()
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/transformers/trainer.py", line 1539, in train
    return inner_training_loop(
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/transformers/trainer.py", line 1568, in _inner_training_loop
    train_dataloader = self.get_train_dataloader()
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/transformers/trainer.py", line 806, in get_train_dataloader
    dataloader_params["sampler"] = self._get_train_sampler()
  File "/scratch/nshaheen/project/may_federated/LLaVA_flowertune/LLaVA/llava/train/llava_trainer.py", line 139, in _get_train_sampler
    if self.args.group_by_modality_length:
AttributeError: 'TrainingArguments' object has no attribute 'group_by_modality_length'

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=852642, ip=10.53.222.32, actor_id=d658ca05287a7be6e99f355501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fa8b981cd90>)
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: 'TrainingArguments' object has no attribute 'group_by_modality_length'

[2024-05-08 14:38:07,316][flwr][ERROR] - [36mray::ClientAppActor.run()[39m (pid=852642, ip=10.53.222.32, actor_id=d658ca05287a7be6e99f355501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fa8b981cd90>)
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/flwr/client/client_app.py", line 98, in __call__
    return self._call(message, context)
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/flwr/client/client_app.py", line 81, in ffn
    out_message = handle_legacy_message_from_msgtype(
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/flwr/client/message_handler/message_handler.py", line 130, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/flwr/client/client.py", line 234, in maybe_call_fit
    return client.fit(fit_ins)
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/flwr/client/numpy_client.py", line 238, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/scratch/nshaheen/project/may_federated/LLaVA_flowertune/client.py", line 84, in fit
    results = trainer.train()
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/transformers/trainer.py", line 1539, in train
    return inner_training_loop(
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/transformers/trainer.py", line 1568, in _inner_training_loop
    train_dataloader = self.get_train_dataloader()
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/transformers/trainer.py", line 806, in get_train_dataloader
    dataloader_params["sampler"] = self._get_train_sampler()
  File "/scratch/nshaheen/project/may_federated/LLaVA_flowertune/LLaVA/llava/train/llava_trainer.py", line 139, in _get_train_sampler
    if self.args.group_by_modality_length:
AttributeError: 'TrainingArguments' object has no attribute 'group_by_modality_length'

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=852642, ip=10.53.222.32, actor_id=d658ca05287a7be6e99f355501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fa8b981cd90>)
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: 'TrainingArguments' object has no attribute 'group_by_modality_length'
[2024-05-08 14:38:07,318][flwr][INFO] - aggregate_fit: received 0 results and 2 failures
[2024-05-08 14:38:07,839][flwr][ERROR] - get_model() missing 2 required positional arguments: 'data_cfg' and 'tokenizer'
[2024-05-08 14:38:07,863][flwr][ERROR] - Traceback (most recent call last):
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/flwr/simulation/app.py", line 308, in start_simulation
    hist = run_fl(
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/flwr/server/server.py", line 483, in run_fl
    hist, elapsed_time = server.fit(
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/flwr/server/server.py", line 126, in fit
    res_cen = self.strategy.evaluate(current_round, parameters=self.parameters)
  File "/home/nshaheen/anaconda3/envs/grad_proj/lib/python3.9/site-packages/flwr/server/strategy/fedavg.py", line 167, in evaluate
    eval_res = self.evaluate_fn(server_round, parameters_ndarrays, {})
  File "/scratch/nshaheen/project/may_federated/LLaVA_flowertune/utils.py", line 16, in evaluate
    model = get_model(model_cfg)
TypeError: get_model() missing 2 required positional arguments: 'data_cfg' and 'tokenizer'

[2024-05-08 14:38:07,864][flwr][ERROR] - Your simulation crashed :(. This could be because of several reasons. The most common are: 
	 > Sometimes, issues in the simulation code itself can cause crashes. It's always a good idea to double-check your code for any potential bugs or inconsistencies that might be contributing to the problem. For example: 
		 - You might be using a class attribute in your clients that hasn't been defined.
		 - There could be an incorrect method call to a 3rd party library (e.g., PyTorch).
		 - The return types of methods in your clients/strategies might be incorrect.
	 > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.
	 > All the actors in your pool crashed. This could be because: 
		 - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 2, 'num_gpus': 1.0} is not enough for your run). Use fewer concurrent actors. 
		 - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 2, 'num_gpus': 1.0}.
Take a look at the Flower simulation examples for guidance <https://flower.ai/docs/framework/how-to-run-simulations.html>.
